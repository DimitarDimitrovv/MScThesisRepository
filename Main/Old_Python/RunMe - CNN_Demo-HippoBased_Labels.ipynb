{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# StackOverflowData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T14:00:23.754565Z",
     "start_time": "2019-06-03T14:00:23.739081Z"
    },
    "code_folding": [
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "# if you have labels\n",
    "\n",
    "\n",
    "'''\n",
    "    The idea of the function below (map_label) is a simplified approach to what they did in the initial paper. \n",
    "    They used the so called Hungarian method in order to optimize the labeling. The optimization is based on the\n",
    "    ground truth and the labels from the k clustering. Using the example dataset:\n",
    "    We have 20 000 items split in 20 categories, which means that a certain category has multiple items. \n",
    "    The idea of best mapping is to take all tuples from ground truth and prediction per item and count their \n",
    "    occurancies. For example (6, 18), 88), 6 being the ground truth and 18 the prediction truth. This \n",
    "    combination has occured 88 times but ((6, 3), 116) has occured 116 times, so at the end the function \n",
    "    will return 6,3 as best map.\n",
    "'''\n",
    "\n",
    "\n",
    "import munkres\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "def translateLabels(masterList, listToConvert):    \n",
    "    contMatrix = contingency_matrix(masterList, listToConvert)\n",
    "    labelMatcher = munkres.Munkres()\n",
    "    labelTranlater = labelMatcher.compute(contMatrix.max() - contMatrix)\n",
    "\n",
    "    uniqueLabels1 = list(set(masterList))\n",
    "    uniqueLabels2 = list(set(listToConvert))\n",
    "\n",
    "    tranlatorDict = {}\n",
    "    for thisPair in labelTranlater:\n",
    "        tranlatorDict[uniqueLabels2[thisPair[1]]] = uniqueLabels1[thisPair[0]]\n",
    "\n",
    "    return [tranlatorDict[label] for label in listToConvert]\n",
    "\n",
    "\n",
    "from munkres import Munkres\n",
    "def best_map(L1,L2):\n",
    "    #L1 should be the groundtruth labels and L2 should be the clustering labels we got\n",
    "    Label1 = np.unique(L1)\n",
    "    nClass1 = len(Label1)\n",
    "    Label2 = np.unique(L2)\n",
    "    nClass2 = len(Label2)\n",
    "    nClass = np.maximum(nClass1,nClass2)\n",
    "    G = np.zeros((nClass,nClass))\n",
    "    for i in range(nClass1):\n",
    "        ind_cla1 = L1 == Label1[i]\n",
    "        ind_cla1 = ind_cla1.astype(float)\n",
    "        for j in range(nClass2):\n",
    "            ind_cla2 = L2 == Label2[j]\n",
    "            ind_cla2 = ind_cla2.astype(float)\n",
    "            G[i,j] = np.sum(ind_cla2 * ind_cla1)\n",
    "    m = Munkres()\n",
    "    index = m.compute(-G.T)\n",
    "    index = np.array(index)\n",
    "    c = index[:,1]\n",
    "    newL2 = np.zeros(L2.shape)\n",
    "    for i in range(nClass2):\n",
    "        newL2[L2 == Label2[i]] = Label1[c[i]]\n",
    "    return newL2 \n",
    "\n",
    "\n",
    "\n",
    "def map_label(true_labels, pred_labels):\n",
    "    label_pair = list(zip(pred_labels, true_labels))\n",
    "    count = tuple(Counter(label_pair).items())  #count the appearance of each pair dict principle\n",
    "    mapping = dict()\n",
    "    n_label = len(np.unique(true_labels))\n",
    "\n",
    "    # map most likely labels from prediction to ground truth\n",
    "\n",
    "    for label in range(n_label):\n",
    "        tuples = [tup for tup in count if tup[0][0] == label]\n",
    "        likely_tuple = max(tuples, key=itemgetter(1))[0] # tuple as input and take the one which appears the most from the list\n",
    "        mapping[likely_tuple[0]] = likely_tuple[1]\n",
    "\n",
    "    pred_labels_mapped = [mapping[x] for x in pred_labels]\n",
    "    return pred_labels_mapped\n",
    "\n",
    "# if you have labels\n",
    "def cluster_quality(true_labels, pred_labels, show=True):\n",
    "    h, c, v = metrics.homogeneity_completeness_v_measure(true_labels, pred_labels)\n",
    "    nmi = metrics.normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    rand = metrics.adjusted_rand_score(true_labels, pred_labels)\n",
    "    pred_labels_mapped = map_label(true_labels, pred_labels) # initial version\n",
    "    #pred_labels_mapped = best_map(true_labels, pred_labels) # 1st test - around 52,5%\n",
    "    #pred_labels_mapped = translateLabels(true_labels, pred_labels) # good results but around 49% with big deviation up to 55,8\n",
    "    acc = metrics.accuracy_score(true_labels, pred_labels_mapped)\n",
    "    '''\n",
    "    if show:\n",
    "        print(\"Homogeneity: %0.3f\" % h)\n",
    "        print(\"Completeness: %0.3f\" % c)\n",
    "        print(\"V-measure: %0.3f\" % v)\n",
    "        print(\"NMI: %0.3f\" % nmi)\n",
    "        print(\"Rand score: %0.3f\" % rand)\n",
    "        print(\"Accuracy: %0.3f\" % acc)\n",
    "        '''\n",
    "    return dict(\n",
    "        homogeneity=round(h,3),\n",
    "        completeness=round(c,3),\n",
    "        vmeasure=round(v,3),\n",
    "        nmi=round(nmi,3),\n",
    "        rand=round(rand,3),\n",
    "        accuracy=round(acc,3),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:40:45.121607Z",
     "start_time": "2019-06-03T13:40:43.262481Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "############################\n",
    "# Loading Data\n",
    "############################\n",
    "\n",
    "EMBEDDING_FILE = 'data/GoogleNews-vectors-negative300.bin' # word vectors\n",
    "text_path = 'data/StackOverflow.txt'# data without labels\n",
    "#text_path = 'data/blq.txt'# data without labels - Own Data\n",
    "label_path = 'data/StackOverflow_gnd.txt' # labels per row in the data file\n",
    "\n",
    "with open(text_path) as f:\n",
    "    data = [text.strip() for text in f]\n",
    "\n",
    "with open(label_path) as f:\n",
    "    target = f.readlines()\n",
    "    \n",
    "target = [int(label.rstrip('\\n')) for label in target] # the data has /n at the end of each row\n",
    "\n",
    "print(\"Total: %s short texts\" % format(len(data), \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:40:47.078858Z",
     "start_time": "2019-06-03T13:40:46.419116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Preprocessing\n",
    "############################\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences_full = tokenizer.texts_to_sequences(data) # replace words/tokens with numbers\n",
    "\n",
    "word_index = tokenizer.word_index # get the coresponing word:number dict format\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "MAX_NB_WORDS = len(word_index) # all words - len\n",
    "\n",
    "seq_lens = [len(s) for s in sequences_full] # get the count of words/symbols in the sequence from above\n",
    "print(\"Minumum length: %d\" % min(seq_lens))\n",
    "print(\"Average length: %d\" % np.mean(seq_lens))\n",
    "print(\"Max length: %d\" % max(seq_lens))\n",
    "MAX_SEQUENCE_LENGTH = max(seq_lens)\n",
    "\n",
    "X = pad_sequences(sequences_full, maxlen=MAX_SEQUENCE_LENGTH) # uses the len of the list and max len\n",
    "y = target # we take the target values\n",
    "\n",
    "'''\n",
    "    pad_sequences is used to ensure that all sequences in a list have the same length. By default this \n",
    "    is done by padding 0 in the beginning of each sequence until each sequence has the same \n",
    "    length as the longest sequence.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:41:46.275336Z",
     "start_time": "2019-06-03T13:40:47.988284Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Preparing embedding matrix\n",
    "############################\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM)) # Init as 0's and updated if in word2vec\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    #else:\n",
    "        #print(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:41:54.830922Z",
     "start_time": "2019-06-03T13:41:46.324020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Preparing target using Average embeddings (AE)\n",
    "#################################################\n",
    "Y = {}\n",
    "tfidf = tokenizer.sequences_to_matrix(sequences_full, mode='tfidf') # converting List of sequences (a sequence is a list of integer word indices).\n",
    "# TF-IDF Acc: 0,329\n",
    "binary_seq = tokenizer.sequences_to_matrix(sequences_full, mode='binary') # Acc: 0,458\n",
    "count_seq = tokenizer.sequences_to_matrix(sequences_full, mode='count') # Acc: 0,467\n",
    "frequency_seq = tokenizer.sequences_to_matrix(sequences_full, mode='freq') # Acc: 0452\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normed = Normalizer().fit_transform(count_seq)\n",
    "average_embeddings = np.dot(normed, embedding_matrix)\n",
    "Y[\"ae\"] = average_embeddings\n",
    "print(\"Shape of average embedding: \", Y['ae'].shape)\n",
    "\n",
    "\n",
    "# binary Y\n",
    "from sklearn import preprocessing\n",
    "reduction_name = \"ae\"\n",
    "B = preprocessing.Binarizer().fit_transform(Y[reduction_name]) # binarizing the whole value list for ae key (which is the only key)\n",
    "\n",
    "# Last dimension in the CNN\n",
    "TARGET_DIM = B.shape[1]\n",
    "\n",
    "# Example of binarized target vector\n",
    "print(B.shape)\n",
    "print(B[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T19:05:24.907100Z",
     "start_time": "2019-05-26T19:05:24.903570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:52:51.381367Z",
     "start_time": "2019-05-26T21:52:51.279893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "# https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:52:57.505703Z",
     "start_time": "2019-05-26T21:52:57.430125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(binary_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:53:01.858512Z",
     "start_time": "2019-05-26T21:53:01.749873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(count_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:53:05.465426Z",
     "start_time": "2019-05-26T21:53:05.382384Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(frequency_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:41:54.925847Z",
     "start_time": "2019-06-03T13:41:54.912386Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###############PLAYING-GridSearch#################################\n",
    "# Playing train model\n",
    "################################################\n",
    "\n",
    "# Best accuracy for now\n",
    "\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation, concatenate, Input\n",
    "from keras.layers import Embedding, Conv1D, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network\n",
    "# the above link has to do with dropout positioning\n",
    "\n",
    "# https://github.com/keras-team/keras/issues/7290\n",
    "# has to do with spatial dropout after embedding layer\n",
    "\n",
    "# https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c\n",
    "# SpatialDropout again\n",
    "\n",
    "def get_model():\n",
    "    embedding_matrix_copy = embedding_matrix.copy()\n",
    "    trainable_embedding = False\n",
    "    #filters = [2,3,4]\n",
    "    filters = [3,4,5]\n",
    "    # Embedding layer\n",
    "    pretrained_embedding_layer = Embedding(\n",
    "        input_dim=nb_words,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "\n",
    "    # Input\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
    "    \n",
    "    # DropOut\n",
    "    #x = Dropout(0.2)(embedded_sequences)\n",
    "    spatial_x = SpatialDropout1D(0.1)(embedded_sequences)\n",
    "    \n",
    "    # 1st Layer\n",
    "    conv_layers = []\n",
    "    for i in filters:\n",
    "        x = Conv1D(125, i, activation='tanh', padding='same')(spatial_x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        #x = Dropout(0.2)(x)\n",
    "        conv_layers.append(x)\n",
    "    merged = concatenate(conv_layers)\n",
    "    # Dense\n",
    "    x = Dense(256,activation = 'tanh')(merged)\n",
    "    \n",
    "    # DropOut\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    # Output\n",
    "    predictions = Dense(TARGET_DIM, activation='sigmoid')(x) \n",
    "    # sigmoid because we want 0,1 for one of the categories\n",
    "    \n",
    "    model = Model(sequence_input, predictions)\n",
    "\n",
    "    model.layers[1].trainable=trainable_embedding #embedding layer\n",
    "\n",
    "    #adam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #standard for keras\n",
    "    #optimizer = Adam(lr=learn_rate)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy','mae'])\n",
    "    \n",
    "    # Fine-tune embeddings or not\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T19:03:37.078522Z",
     "start_time": "2019-05-26T19:03:31.893479Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grid Search with best model\n",
    "# Use scikit-learn to grid search the number of neurons\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=get_model, epochs=50, batch_size=100,verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "\n",
    "#activ = ['relu', 'tanh', 'sigmoid']\n",
    "#optimizers = ['Nadam','Adam']\n",
    "droupou = [0.1,0.2,0.4,0.5,0.6]\n",
    "#init_mode = ['uniform', 'lecun_uniform']\n",
    "#filters =[100,120]\n",
    "#size = [100,120,125]\n",
    "#filterme = [[3,4,5], [2,3,4]]\n",
    "#batch_size = [200]\n",
    "#epochs = [120]\n",
    "#learn_rate = [0.001, 0.01, 0.02, 0.1]\n",
    "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "param_grid = dict(droupout = droupou)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs= -1)\n",
    "grid_result = grid.fit(X, B)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:56:47.452766Z",
     "start_time": "2019-06-03T13:41:54.978265Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 25 epoch and 80 batch = 50.3 accuracy\n",
    "nb_epoch = 50\n",
    "checkpoint = ModelCheckpoint('models/weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "model = get_model()\n",
    "history = model.fit(X, B, validation_split=0.2,\n",
    "              epochs=nb_epoch, batch_size=100, verbose=1, shuffle=True)\n",
    "\n",
    "# create model that gives penultimate layer (предпоследно)\n",
    "input1 = model.layers[0].input\n",
    "output = model.layers[-2].output\n",
    "\n",
    "model_penultimate = Model(input1, output)\n",
    "\n",
    "# inference of penultimate layer\n",
    "H = model_penultimate.predict(X)\n",
    "print(\"Sample shape: {}\".format(H.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:56:47.879656Z",
     "start_time": "2019-06-03T13:56:47.536958Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:56:48.228501Z",
     "start_time": "2019-06-03T13:56:47.966651Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T14:00:34.354568Z",
     "start_time": "2019-06-03T14:00:31.015495Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_labels = y\n",
    "n_clusters = len(np.unique(y))\n",
    "#n_clusters = 27 # for own data\n",
    "print(\"Number of classes: %d\" % n_clusters)\n",
    "km = KMeans(n_clusters=n_clusters, n_jobs=10)\n",
    "result = dict()\n",
    "V = normalize(H, norm='l2')\n",
    "km.fit(V)\n",
    "pred = km.labels_\n",
    "pred1 = km.cluster_centers_\n",
    "print(pred)\n",
    "a = cluster_quality(true_labels, pred)\n",
    "#np.save(\"pred.npy\", pred)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Running clusterS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:49:14.314360Z",
     "start_time": "2019-05-26T21:45:21.772346Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# clustering loop\n",
    "true_labels = y\n",
    "n_clusters = len(np.unique(y))\n",
    "#n_clusters = 27 # for own data\n",
    "#print(\"Number of classes: %d\" % n_clusters)\n",
    "km = KMeans(n_clusters=n_clusters, n_jobs=10)\n",
    "result = dict()\n",
    "V = normalize(H, norm='l2')\n",
    "df = pd.DataFrame()\n",
    "for i in range(50):\n",
    "    km.fit(V)\n",
    "    pred = km.labels_\n",
    "    pred1 = km.cluster_centers_\n",
    "    #print(pred)\n",
    "    a = cluster_quality(true_labels, pred)\n",
    "    df = df.append([a],ignore_index=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:49:15.491147Z",
     "start_time": "2019-05-26T21:49:15.480939Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# round up this and the ones below in function to end up with a table for the paper\n",
    "dfToList = df['accuracy'].tolist()\n",
    "lenList = len(dfToList)\n",
    "sumMe = 0\n",
    "for i in dfToList:\n",
    "    sumMe += i\n",
    "df['average_acc'] = round(sumMe/lenList,3)\n",
    "df['diff_acc'] = df.apply(lambda x: (x['accuracy'] - x['average_acc']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T20:52:06.620773Z",
     "start_time": "2019-05-26T20:52:06.595644Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# using tfidf\n",
    "df_tfidf = df\n",
    "max_acc = df_tfidf['accuracy'].max() # 0,581\n",
    "print('Max accuracy is: {}'.format(max_acc))\n",
    "min_acc = df_tfidf['accuracy'].min() # 0.463\n",
    "print('Min accuracy is: {}'.format(min_acc))\n",
    "average_acc = df_tfidf['average_acc'].max() # 0.514\n",
    "print('Average accuracy is: {}'.format(average_acc))\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:12:49.039736Z",
     "start_time": "2019-05-26T21:12:49.009654Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# using count\n",
    "df_count = df\n",
    "max_acc = df_count['accuracy'].max() # 0,581\n",
    "print('Max accuracy is: {}'.format(max_acc))\n",
    "min_acc = df_count['accuracy'].min() # 0.463\n",
    "print('Min accuracy is: {}'.format(min_acc))\n",
    "average_acc = df_count['average_acc'].max() # 0.514\n",
    "print('Average accuracy is: {}'.format(average_acc))\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:30:54.962992Z",
     "start_time": "2019-05-26T21:30:54.927466Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# using frequency\n",
    "df_freq = df\n",
    "max_acc = df_freq['accuracy'].max() # 0,581\n",
    "print('Max accuracy is: {}'.format(max_acc))\n",
    "min_acc = df_freq['accuracy'].min() # 0.463\n",
    "print('Min accuracy is: {}'.format(min_acc))\n",
    "average_acc = df_freq['average_acc'].max() # 0.514\n",
    "print('Average accuracy is: {}'.format(average_acc))\n",
    "df_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T21:49:16.655310Z",
     "start_time": "2019-05-26T21:49:16.620997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# using binary\n",
    "df_binary = df\n",
    "max_acc = df_binary['accuracy'].max() # 0,581\n",
    "print('Max accuracy is: {}'.format(max_acc))\n",
    "min_acc = df_binary['accuracy'].min() # 0.463\n",
    "print('Min accuracy is: {}'.format(min_acc))\n",
    "average_acc = df_binary['average_acc'].max() # 0.514\n",
    "print('Average accuracy is: {}'.format(average_acc))\n",
    "df_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Save/Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T10:54:50.827159Z",
     "start_time": "2019-06-07T10:54:50.043204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# Saving Model and weiths\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"hippo_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"hippo_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('hippo_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"hippo_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HippoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:16:28.509635Z",
     "start_time": "2019-06-16T17:16:27.663724Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "# if you have labels\n",
    "\n",
    "\n",
    "'''\n",
    "    The idea of the function below (map_label) is a simplified approach to what they did in the initial paper. \n",
    "    They used the so called Hungarian method in order to optimize the labeling. The optimization is based on the\n",
    "    ground truth and the labels from the k clustering. Using the example dataset:\n",
    "    We have 20 000 items split in 20 categories, which means that a certain category has multiple items. \n",
    "    The idea of best mapping is to take all tuples from ground truth and prediction per item and count their \n",
    "    occurancies. For example (6, 18), 88), 6 being the ground truth and 18 the prediction truth. This \n",
    "    combination has occured 88 times but ((6, 3), 116) has occured 116 times, so at the end the function \n",
    "    will return 6,3 as best map.\n",
    "'''\n",
    "\n",
    "def map_label(true_labels, pred_labels):\n",
    "    label_pair = list(zip(pred_labels, true_labels))\n",
    "    count = tuple(Counter(label_pair).items())  #count the appearance of each pair dict principle\n",
    "    mapping = dict()\n",
    "    n_label = len(np.unique(true_labels))\n",
    "\n",
    "    # map most likely labels from prediction to ground truth\n",
    "\n",
    "    for label in range(n_label):\n",
    "        tuples = [tup for tup in count if tup[0][0] == label]\n",
    "        likely_tuple = max(tuples, key=itemgetter(1))[0] # tuple as input and take the one which appears the most from the list\n",
    "        mapping[likely_tuple[0]] = likely_tuple[1]\n",
    "\n",
    "    pred_labels_mapped = [mapping[x] for x in pred_labels]\n",
    "    return pred_labels_mapped\n",
    "\n",
    "# if you have labels\n",
    "def cluster_quality(true_labels, pred_labels, show=True):\n",
    "    h, c, v = metrics.homogeneity_completeness_v_measure(true_labels, pred_labels)\n",
    "    nmi = metrics.normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    rand = metrics.adjusted_rand_score(true_labels, pred_labels)\n",
    "    pred_labels_mapped = map_label(true_labels, pred_labels)\n",
    "    acc = metrics.accuracy_score(true_labels, pred_labels_mapped)\n",
    "    '''\n",
    "    # Prints the scores\n",
    "    if show:\n",
    "        print(\"Homogeneity: %0.3f\" % h)\n",
    "        print(\"Completeness: %0.3f\" % c)\n",
    "        print(\"V-measure: %0.3f\" % v)\n",
    "        print(\"NMI: %0.3f\" % nmi)\n",
    "        print(\"Rand score: %0.3f\" % rand)\n",
    "        print(\"Accuracy: %0.3f\" % acc)\n",
    "        '''\n",
    "    return dict(\n",
    "        homogeneity=round(h,3),\n",
    "        completeness=round(c,3),\n",
    "        vmeasure=round(v,3),\n",
    "        nmi=round(nmi,3),\n",
    "        rand=round(rand,3),\n",
    "        accuracy=round(acc,3),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:16:30.628008Z",
     "start_time": "2019-06-16T17:16:28.516057Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 20,000 short texts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "############################\n",
    "# Loading Data\n",
    "############################\n",
    "\n",
    "EMBEDDING_FILE = 'data/GoogleNews-vectors-negative300.bin' # word vectors\n",
    "text_path = 'data/StackOverflow.txt'# data without labels\n",
    "#text_path = 'data/data_NN.txt'# data without labels - Own Data\n",
    "#text_path = 'data/data_NN_Scrapped.txt'# data without labels - Own Data based on scrapping\n",
    "label_path = 'data/StackOverflow_gnd.txt' # labels per row in the data file\n",
    "\n",
    "with open(text_path) as f:\n",
    "    data = [text.strip() for text in f]\n",
    "\n",
    "with open(label_path) as f:\n",
    "    target = f.readlines()\n",
    "    \n",
    "target = [int(label.rstrip('\\n')) for label in target] # the data has /n at the end of each row\n",
    "\n",
    "print(\"Total: %s short texts\" % format(len(data), \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:16:31.288694Z",
     "start_time": "2019-06-16T17:16:30.643668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11365 unique tokens.\n",
      "Minumum length: 1\n",
      "Average length: 8\n",
      "Max length: 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    pad_sequences is used to ensure that all sequences in a list have the same length. By default this \\n    is done by padding 0 in the beginning of each sequence until each sequence has the same \\n    length as the longest sequence.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# Preprocessing\n",
    "############################\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences_full = tokenizer.texts_to_sequences(data) # replace words/tokens with numbers\n",
    "\n",
    "word_index = tokenizer.word_index # get the coresponing word:number dict format\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "MAX_NB_WORDS = len(word_index) # all words - len\n",
    "\n",
    "seq_lens = [len(s) for s in sequences_full] # get the count of words/symbols in the sequence from above\n",
    "print(\"Minumum length: %d\" % min(seq_lens))\n",
    "print(\"Average length: %d\" % np.mean(seq_lens))\n",
    "print(\"Max length: %d\" % max(seq_lens))\n",
    "MAX_SEQUENCE_LENGTH = max(seq_lens)\n",
    "\n",
    "X = pad_sequences(sequences_full, maxlen=MAX_SEQUENCE_LENGTH) # uses the len of the list and max len\n",
    "y = target # we take the target values\n",
    "\n",
    "'''\n",
    "    pad_sequences is used to ensure that all sequences in a list have the same length. By default this \n",
    "    is done by padding 0 in the beginning of each sequence until each sequence has the same \n",
    "    length as the longest sequence.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:17:23.017232Z",
     "start_time": "2019-06-16T17:16:31.333898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4269\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Preparing embedding matrix\n",
    "############################\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM)) # Init as 0's and updated if in word2vec\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    #else:\n",
    "        #print(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:17:30.954770Z",
     "start_time": "2019-06-16T17:17:23.077588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of average embedding:  (20000, 300)\n",
      "(20000, 300)\n",
      "[1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# Preparing target using Average embeddings (AE)\n",
    "#################################################\n",
    "Y = {}\n",
    "tfidf_seq = tokenizer.sequences_to_matrix(sequences_full, mode='tfidf') # converting List of sequences (a sequence is a list of integer word indices).\n",
    "# TF-IDF Acc: 0,329\n",
    "binary_seq = tokenizer.sequences_to_matrix(sequences_full, mode='binary') # Acc: 0,458\n",
    "count_seq = tokenizer.sequences_to_matrix(sequences_full, mode='count') # Acc: 0,467\n",
    "frequency_seq = tokenizer.sequences_to_matrix(sequences_full, mode='freq') # Acc: 0452\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normed_value = Normalizer().fit_transform(count_seq)\n",
    "average_embeddings = np.dot(normed_value, embedding_matrix)\n",
    "Y[\"ae\"] = average_embeddings\n",
    "print(\"Shape of average embedding: \", Y['ae'].shape)\n",
    "\n",
    "\n",
    "# binary Y\n",
    "from sklearn import preprocessing\n",
    "reduction_name = \"ae\"\n",
    "B = preprocessing.Binarizer().fit_transform(Y[reduction_name]) # binarizing the whole value list for ae key (which is the only key)\n",
    "\n",
    "# Shape of last dimension in the CNN\n",
    "TARGET_DIM = B.shape[1]\n",
    "\n",
    "# Example of binarized target vector\n",
    "print(B.shape)\n",
    "print(B[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:54:49.794912Z",
     "start_time": "2019-06-16T17:54:49.784625Z"
    }
   },
   "outputs": [],
   "source": [
    "###############PLAYING-GridSearch#################################\n",
    "# Playing train model\n",
    "################################################\n",
    "\n",
    "# Best accuracy for now\n",
    "\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation, concatenate, Input\n",
    "from keras.layers import Embedding, Conv1D, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network\n",
    "# the above link has to do with dropout positioning\n",
    "\n",
    "# https://github.com/keras-team/keras/issues/7290\n",
    "# has to do with spatial dropout after embedding layer\n",
    "\n",
    "# https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c\n",
    "# SpatialDropout again\n",
    "\n",
    "def get_model():\n",
    "    embedding_matrix_copy = embedding_matrix.copy()\n",
    "    trainable_embedding = False\n",
    "    #filters = [2,3,4]\n",
    "    filters = [3,4,5]\n",
    "    \n",
    "    # Embedding layer\n",
    "    pretrained_embedding_layer = Embedding(\n",
    "        input_dim=nb_words,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "\n",
    "    # Input\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
    "    \n",
    "    # DropOut\n",
    "    #x = Dropout(0.2)(embedded_sequences)\n",
    "    spatial_x = SpatialDropout1D(0.1)(embedded_sequences)\n",
    "    \n",
    "    # 1st Layer\n",
    "    conv_layers = []\n",
    "    for i in filters:\n",
    "        x = Conv1D(125, i, activation='tanh', padding='same')(spatial_x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        #x = Dropout(0.2)(x)\n",
    "        conv_layers.append(x)\n",
    "    merged = concatenate(conv_layers)\n",
    "    \n",
    "    # Dense\n",
    "    #x = Dense(TARGET_DIM,activation = 'tanh')(merged)\n",
    "    x = Dense(256,activation = 'tanh')(merged) # best performing\n",
    "    \n",
    "    # DropOut\n",
    "    x = Dropout(0.1)(x)\n",
    "    #x = Dropout(0.1)(merged) # best performing\n",
    "    \n",
    "    # Output\n",
    "    predictions = Dense(TARGET_DIM, activation='sigmoid')(x) \n",
    "    # sigmoid because we want 0,1 for one of the categories\n",
    "    \n",
    "    model = Model(sequence_input, predictions)\n",
    "\n",
    "    model.layers[1].trainable=trainable_embedding #embedding layer\n",
    "\n",
    "    #adam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #standard for keras\n",
    "    #optimizer = Adam(lr=learn_rate)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy','mae'])\n",
    "    \n",
    "    # Fine-tune embeddings or not\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_plot_no_shapes.png', show_shapes=False, show_layer_names=True)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:07:32.827898Z",
     "start_time": "2019-06-16T17:54:51.679600Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 34, 300)      3409800     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 34, 300)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 34, 125)      112625      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 34, 125)      150125      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 34, 125)      187625      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 125)          0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 125)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 125)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 375)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          96256       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 300)          77100       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,033,531\n",
      "Trainable params: 623,731\n",
      "Non-trainable params: 3,409,800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "16000/16000 [==============================] - 13s 811us/step - loss: 0.4948 - acc: 0.7554 - mean_absolute_error: 0.3361 - val_loss: 0.4472 - val_acc: 0.7886 - val_mean_absolute_error: 0.3062\n",
      "Epoch 2/50\n",
      "16000/16000 [==============================] - 13s 801us/step - loss: 0.4129 - acc: 0.8078 - mean_absolute_error: 0.2799 - val_loss: 0.4022 - val_acc: 0.8145 - val_mean_absolute_error: 0.2731\n",
      "Epoch 3/50\n",
      "16000/16000 [==============================] - 14s 846us/step - loss: 0.3768 - acc: 0.8280 - mean_absolute_error: 0.2545 - val_loss: 0.3757 - val_acc: 0.8284 - val_mean_absolute_error: 0.2523\n",
      "Epoch 4/50\n",
      "16000/16000 [==============================] - 14s 851us/step - loss: 0.3540 - acc: 0.8401 - mean_absolute_error: 0.2381 - val_loss: 0.3580 - val_acc: 0.8375 - val_mean_absolute_error: 0.2404\n",
      "Epoch 5/50\n",
      "16000/16000 [==============================] - 14s 857us/step - loss: 0.3376 - acc: 0.8485 - mean_absolute_error: 0.2262 - val_loss: 0.3460 - val_acc: 0.8431 - val_mean_absolute_error: 0.2294\n",
      "Epoch 6/50\n",
      "16000/16000 [==============================] - 14s 860us/step - loss: 0.3250 - acc: 0.8550 - mean_absolute_error: 0.2170 - val_loss: 0.3373 - val_acc: 0.8475 - val_mean_absolute_error: 0.2219\n",
      "Epoch 7/50\n",
      "16000/16000 [==============================] - 14s 881us/step - loss: 0.3153 - acc: 0.8597 - mean_absolute_error: 0.2098 - val_loss: 0.3299 - val_acc: 0.8515 - val_mean_absolute_error: 0.2181\n",
      "Epoch 8/50\n",
      "16000/16000 [==============================] - 14s 872us/step - loss: 0.3071 - acc: 0.8639 - mean_absolute_error: 0.2035 - val_loss: 0.3240 - val_acc: 0.8541 - val_mean_absolute_error: 0.2128\n",
      "Epoch 9/50\n",
      "16000/16000 [==============================] - 14s 880us/step - loss: 0.3003 - acc: 0.8670 - mean_absolute_error: 0.1985 - val_loss: 0.3193 - val_acc: 0.8563 - val_mean_absolute_error: 0.2079\n",
      "Epoch 10/50\n",
      "16000/16000 [==============================] - 14s 887us/step - loss: 0.2948 - acc: 0.8698 - mean_absolute_error: 0.1942 - val_loss: 0.3159 - val_acc: 0.8578 - val_mean_absolute_error: 0.2058\n",
      "Epoch 11/50\n",
      "16000/16000 [==============================] - 14s 890us/step - loss: 0.2896 - acc: 0.8725 - mean_absolute_error: 0.1903 - val_loss: 0.3124 - val_acc: 0.8596 - val_mean_absolute_error: 0.2013\n",
      "Epoch 12/50\n",
      "16000/16000 [==============================] - 15s 921us/step - loss: 0.2855 - acc: 0.8742 - mean_absolute_error: 0.1871 - val_loss: 0.3100 - val_acc: 0.8606 - val_mean_absolute_error: 0.1996\n",
      "Epoch 13/50\n",
      "16000/16000 [==============================] - 16s 989us/step - loss: 0.2817 - acc: 0.8762 - mean_absolute_error: 0.1842 - val_loss: 0.3075 - val_acc: 0.8619 - val_mean_absolute_error: 0.1967\n",
      "Epoch 14/50\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.2779 - acc: 0.8778 - mean_absolute_error: 0.1815 - val_loss: 0.3053 - val_acc: 0.8630 - val_mean_absolute_error: 0.1931\n",
      "Epoch 15/50\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.2750 - acc: 0.8793 - mean_absolute_error: 0.1792 - val_loss: 0.3035 - val_acc: 0.8640 - val_mean_absolute_error: 0.1926\n",
      "Epoch 16/50\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.2720 - acc: 0.8808 - mean_absolute_error: 0.1769 - val_loss: 0.3022 - val_acc: 0.8647 - val_mean_absolute_error: 0.1908\n",
      "Epoch 17/50\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.2695 - acc: 0.8820 - mean_absolute_error: 0.1749 - val_loss: 0.3009 - val_acc: 0.8654 - val_mean_absolute_error: 0.1876\n",
      "Epoch 18/50\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.2671 - acc: 0.8830 - mean_absolute_error: 0.1731 - val_loss: 0.2989 - val_acc: 0.8661 - val_mean_absolute_error: 0.1872\n",
      "Epoch 19/50\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.2652 - acc: 0.8841 - mean_absolute_error: 0.1715 - val_loss: 0.2981 - val_acc: 0.8665 - val_mean_absolute_error: 0.1856\n",
      "Epoch 20/50\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.2631 - acc: 0.8850 - mean_absolute_error: 0.1700 - val_loss: 0.2970 - val_acc: 0.8669 - val_mean_absolute_error: 0.1850\n",
      "Epoch 21/50\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.2614 - acc: 0.8855 - mean_absolute_error: 0.1686 - val_loss: 0.2960 - val_acc: 0.8674 - val_mean_absolute_error: 0.1845\n",
      "Epoch 22/50\n",
      "16000/16000 [==============================] - 15s 930us/step - loss: 0.2592 - acc: 0.8866 - mean_absolute_error: 0.1671 - val_loss: 0.2955 - val_acc: 0.8678 - val_mean_absolute_error: 0.1846\n",
      "Epoch 23/50\n",
      "16000/16000 [==============================] - 15s 910us/step - loss: 0.2577 - acc: 0.8874 - mean_absolute_error: 0.1659 - val_loss: 0.2943 - val_acc: 0.8684 - val_mean_absolute_error: 0.1834\n",
      "Epoch 24/50\n",
      "16000/16000 [==============================] - 15s 921us/step - loss: 0.2561 - acc: 0.8882 - mean_absolute_error: 0.1646 - val_loss: 0.2936 - val_acc: 0.8688 - val_mean_absolute_error: 0.1812\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 15s 919us/step - loss: 0.2547 - acc: 0.8887 - mean_absolute_error: 0.1636 - val_loss: 0.2924 - val_acc: 0.8692 - val_mean_absolute_error: 0.1807\n",
      "Epoch 26/50\n",
      "16000/16000 [==============================] - 14s 887us/step - loss: 0.2535 - acc: 0.8894 - mean_absolute_error: 0.1626 - val_loss: 0.2918 - val_acc: 0.8695 - val_mean_absolute_error: 0.1798\n",
      "Epoch 27/50\n",
      "16000/16000 [==============================] - 14s 904us/step - loss: 0.2522 - acc: 0.8901 - mean_absolute_error: 0.1615 - val_loss: 0.2909 - val_acc: 0.8698 - val_mean_absolute_error: 0.1796\n",
      "Epoch 28/50\n",
      "16000/16000 [==============================] - 14s 902us/step - loss: 0.2508 - acc: 0.8906 - mean_absolute_error: 0.1605 - val_loss: 0.2906 - val_acc: 0.8703 - val_mean_absolute_error: 0.1782\n",
      "Epoch 29/50\n",
      "16000/16000 [==============================] - 15s 919us/step - loss: 0.2500 - acc: 0.8908 - mean_absolute_error: 0.1598 - val_loss: 0.2898 - val_acc: 0.8708 - val_mean_absolute_error: 0.1774\n",
      "Epoch 30/50\n",
      "16000/16000 [==============================] - 15s 907us/step - loss: 0.2486 - acc: 0.8917 - mean_absolute_error: 0.1589 - val_loss: 0.2892 - val_acc: 0.8708 - val_mean_absolute_error: 0.1774\n",
      "Epoch 31/50\n",
      "16000/16000 [==============================] - 15s 916us/step - loss: 0.2476 - acc: 0.8922 - mean_absolute_error: 0.1581 - val_loss: 0.2886 - val_acc: 0.8711 - val_mean_absolute_error: 0.1765\n",
      "Epoch 32/50\n",
      "16000/16000 [==============================] - 15s 929us/step - loss: 0.2466 - acc: 0.8926 - mean_absolute_error: 0.1573 - val_loss: 0.2884 - val_acc: 0.8718 - val_mean_absolute_error: 0.1743\n",
      "Epoch 33/50\n",
      "16000/16000 [==============================] - 15s 946us/step - loss: 0.2456 - acc: 0.8930 - mean_absolute_error: 0.1566 - val_loss: 0.2875 - val_acc: 0.8720 - val_mean_absolute_error: 0.1746\n",
      "Epoch 34/50\n",
      "16000/16000 [==============================] - 15s 914us/step - loss: 0.2449 - acc: 0.8934 - mean_absolute_error: 0.1560 - val_loss: 0.2870 - val_acc: 0.8721 - val_mean_absolute_error: 0.1743\n",
      "Epoch 35/50\n",
      "16000/16000 [==============================] - 15s 946us/step - loss: 0.2440 - acc: 0.8939 - mean_absolute_error: 0.1553 - val_loss: 0.2865 - val_acc: 0.8722 - val_mean_absolute_error: 0.1738\n",
      "Epoch 36/50\n",
      "16000/16000 [==============================] - 15s 914us/step - loss: 0.2429 - acc: 0.8943 - mean_absolute_error: 0.1546 - val_loss: 0.2860 - val_acc: 0.8724 - val_mean_absolute_error: 0.1733\n",
      "Epoch 37/50\n",
      "16000/16000 [==============================] - 15s 928us/step - loss: 0.2421 - acc: 0.8947 - mean_absolute_error: 0.1539 - val_loss: 0.2858 - val_acc: 0.8727 - val_mean_absolute_error: 0.1720\n",
      "Epoch 38/50\n",
      "16000/16000 [==============================] - 15s 917us/step - loss: 0.2411 - acc: 0.8951 - mean_absolute_error: 0.1533 - val_loss: 0.2853 - val_acc: 0.8729 - val_mean_absolute_error: 0.1715\n",
      "Epoch 39/50\n",
      "16000/16000 [==============================] - 15s 931us/step - loss: 0.2406 - acc: 0.8954 - mean_absolute_error: 0.1528 - val_loss: 0.2847 - val_acc: 0.8732 - val_mean_absolute_error: 0.1725\n",
      "Epoch 40/50\n",
      "16000/16000 [==============================] - 15s 958us/step - loss: 0.2397 - acc: 0.8957 - mean_absolute_error: 0.1522 - val_loss: 0.2848 - val_acc: 0.8731 - val_mean_absolute_error: 0.1716\n",
      "Epoch 41/50\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.2392 - acc: 0.8960 - mean_absolute_error: 0.1517 - val_loss: 0.2839 - val_acc: 0.8738 - val_mean_absolute_error: 0.1704\n",
      "Epoch 42/50\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.2383 - acc: 0.8963 - mean_absolute_error: 0.1511 - val_loss: 0.2835 - val_acc: 0.8740 - val_mean_absolute_error: 0.1706\n",
      "Epoch 43/50\n",
      "16000/16000 [==============================] - 16s 977us/step - loss: 0.2377 - acc: 0.8966 - mean_absolute_error: 0.1507 - val_loss: 0.2833 - val_acc: 0.8740 - val_mean_absolute_error: 0.1698\n",
      "Epoch 44/50\n",
      "16000/16000 [==============================] - 16s 992us/step - loss: 0.2373 - acc: 0.8968 - mean_absolute_error: 0.1503 - val_loss: 0.2837 - val_acc: 0.8742 - val_mean_absolute_error: 0.1686\n",
      "Epoch 45/50\n",
      "16000/16000 [==============================] - 16s 994us/step - loss: 0.2364 - acc: 0.8973 - mean_absolute_error: 0.1497 - val_loss: 0.2823 - val_acc: 0.8743 - val_mean_absolute_error: 0.1701\n",
      "Epoch 46/50\n",
      "16000/16000 [==============================] - 15s 967us/step - loss: 0.2359 - acc: 0.8975 - mean_absolute_error: 0.1493 - val_loss: 0.2821 - val_acc: 0.8746 - val_mean_absolute_error: 0.1691\n",
      "Epoch 47/50\n",
      "16000/16000 [==============================] - 16s 983us/step - loss: 0.2354 - acc: 0.8978 - mean_absolute_error: 0.1489 - val_loss: 0.2819 - val_acc: 0.8747 - val_mean_absolute_error: 0.1686\n",
      "Epoch 48/50\n",
      "16000/16000 [==============================] - 16s 996us/step - loss: 0.2346 - acc: 0.8981 - mean_absolute_error: 0.1483 - val_loss: 0.2813 - val_acc: 0.8750 - val_mean_absolute_error: 0.1687\n",
      "Epoch 49/50\n",
      "16000/16000 [==============================] - 16s 977us/step - loss: 0.2341 - acc: 0.8983 - mean_absolute_error: 0.1480 - val_loss: 0.2811 - val_acc: 0.8751 - val_mean_absolute_error: 0.1679\n",
      "Epoch 50/50\n",
      "16000/16000 [==============================] - 16s 995us/step - loss: 0.2338 - acc: 0.8985 - mean_absolute_error: 0.1476 - val_loss: 0.2812 - val_acc: 0.8753 - val_mean_absolute_error: 0.1680\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "# 25 epoch and 80 batch = 50.3 accuracy\n",
    "nb_epoch = 50 #changed from 50 cause of time limitations\n",
    "\n",
    "# 5 epochs take 1 hour and 14 mins to load\n",
    "\n",
    "checkpoint = ModelCheckpoint('models/weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "model = get_model()\n",
    "history = model.fit(X, B, validation_split=0.2,\n",
    "              epochs=nb_epoch, batch_size=100, verbose=1, shuffle=True)\n",
    "\n",
    "# create model that gives penultimate layer (предпоследно)\n",
    "input1 = model.layers[0].input\n",
    "output = model.layers[-2].output\n",
    "\n",
    "model_penultimate = Model(input1, output)\n",
    "\n",
    "\n",
    "# inference of penultimate layer\n",
    "#H_penul = model_penultimate.predict(X)\n",
    "#print(\"Sample shape: {}\".format(H_penul.shape))\n",
    "#V_penul = normalize(H_penul, norm='l2') # nomr\n",
    "# standard model prediction\n",
    "#H_standard = model.predict(X)\n",
    "#print(\"Sample shape for standard: {}\".format(H_standard.shape))\n",
    "#V_standard = normalize(H_standard, norm='l2') # norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:07:39.466712Z",
     "start_time": "2019-06-16T18:07:32.910707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (20000, 256)\n"
     ]
    }
   ],
   "source": [
    "# inference of penultimate layer\n",
    "H_penul = model_penultimate.predict(X)\n",
    "print(\"Sample shape: {}\".format(H_penul.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:53:56.255305Z",
     "start_time": "2019-06-16T17:53:56.111008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T17:54:05.222966Z",
     "start_time": "2019-06-16T17:54:05.088896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T09:16:13.942288Z",
     "start_time": "2019-06-07T09:15:24.268373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Amount of Clusters based on Penul data\n",
    "# Wherever it makes the curve switch - around 6-7 for this data\n",
    "\n",
    "import numpy as np\n",
    "from scipy import cluster\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#V = normalize(H, norm='l2') # https://machinelearningmastery.com/vector-norms-machine-learning/\n",
    "#plot variance for each value for 'k' between 1,10\n",
    "initial = [cluster.vq.kmeans(V_penul,i) for i in range(1,40)]\n",
    "pyplot.plot([var for (cent,var) in initial])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T09:17:07.966555Z",
     "start_time": "2019-06-07T09:16:14.447265Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Amount of Clusters based on standard prediction\n",
    "# Wherever it makes the curve switch - around 6-7 for this data\n",
    "\n",
    "import numpy as np\n",
    "from scipy import cluster\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#V = normalize(H, norm='l2') # https://machinelearningmastery.com/vector-norms-machine-learning/\n",
    "#plot variance for each value for 'k' between 1,10\n",
    "initial = [cluster.vq.kmeans(V_standard,i) for i in range(1,40)]\n",
    "pyplot.plot([var for (cent,var) in initial])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:08:14.138108Z",
     "start_time": "2019-06-16T18:08:11.029314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 20\n",
      "[14 14  8 ...  9  9  0]\n",
      "{'homogeneity': 0.507, 'completeness': 0.535, 'vmeasure': 0.521, 'nmi': 0.521, 'rand': 0.318, 'accuracy': 0.569}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dimitar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# First run\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_labels = y\n",
    "n_clusters = len(np.unique(y))\n",
    "#n_clusters = 7 # for own data\n",
    "print(\"Number of classes: %d\" % n_clusters)\n",
    "km = KMeans(n_clusters=n_clusters, n_jobs=10)\n",
    "result_penul = dict()\n",
    "V_penul = normalize(H_penul, norm='l2') # https://machinelearningmastery.com/vector-norms-machine-learning/\n",
    "km.fit(V_penul)\n",
    "pred_penul = km.labels_\n",
    "pred1_penul = km.cluster_centers_\n",
    "print(pred_penul)\n",
    "a = cluster_quality(true_labels, pred_penul) #needs labels\n",
    "#np.save(\"pred.npy\", pred)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T09:17:36.718294Z",
     "start_time": "2019-06-07T09:17:36.286655Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#true_labels = y\n",
    "#n_clusters = len(np.unique(y))\n",
    "n_clusters = 7 # for own data\n",
    "print(\"Number of classes: %d\" % n_clusters)\n",
    "km = KMeans(n_clusters=n_clusters, n_jobs=10)\n",
    "result_standard = dict()\n",
    "#V = normalize(H_penul, norm='l2') # https://machinelearningmastery.com/vector-norms-machine-learning/\n",
    "km.fit(V_standard)\n",
    "pred_standard = km.labels_\n",
    "pred1_standard = km.cluster_centers_\n",
    "print(pred_standard)\n",
    "#a = cluster_quality(true_labels, pred) #needs labels\n",
    "#np.save(\"pred.npy\", pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualization - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "################Visualization####################\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:57:30.498376Z",
     "start_time": "2019-05-28T10:57:30.127993Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_kmeans = km.predict(V)\n",
    "\n",
    "plt.scatter(V[:, 0], V[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=13, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T09:48:49.494745Z",
     "start_time": "2019-06-03T09:48:49.461097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(a, 13) #gives error for some reason\n",
    "plt.scatter(a[:, 0], a[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T09:18:41.689095Z",
     "start_time": "2019-06-07T09:18:18.474414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# DBSCAN for penul layer based on scrapped keywords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "\n",
    "X = StandardScaler().fit_transform(V_penul)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples= 10).fit(V_penul)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(pred_penul, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(pred_penul, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(pred_penul, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(pred_penul, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(pred_penul, labels,\n",
    "                                           average_method='arithmetic'))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result - DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    #plt.plot(xy[:, 0], xy[:, 1], 'o',\n",
    "     #        markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T09:19:10.263407Z",
     "start_time": "2019-06-07T09:18:41.936896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# DBSCAN for standard prediction based on scrapped keywords\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "\n",
    "X = StandardScaler().fit_transform(V_standard)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples= 10).fit(V_standard)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(pred_standard, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(pred_standard, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(pred_standard, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(pred_standard, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(pred_standard, labels,\n",
    "                                           average_method='arithmetic'))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    #plt.plot(xy[:, 0], xy[:, 1], 'o',\n",
    "     #        markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "################Evaluation####################\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T09:54:56.679780Z",
     "start_time": "2019-06-07T09:21:39.271033Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Running the results from standard prediction based on Scrapped Keywords\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "#print(__doc__)\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "\n",
    "pp = PdfPages('ScrappedKeywords_StandardPrediction200.pdf')\n",
    "\n",
    "\n",
    "range_n_clusters = []\n",
    "for i in range(2,201): # make a list from 1 till 31\n",
    "    range_n_clusters.append(i)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(V_standard) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(V_standard)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(V_standard, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(V_standard, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(V_standard[:, 0], V_standard[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = {} with Average Silhouette of: {}\".format(n_clusters,silhouette_avg)),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    pp.savefig(fig)\n",
    "\n",
    "plt.show()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T10:26:08.339840Z",
     "start_time": "2019-06-07T09:54:56.956984Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Running the results from penul layer from Scrappe Keywords\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "#print(__doc__)\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "\n",
    "pp = PdfPages('ScrappedKeywords_PenulLayer200.pdf')\n",
    "\n",
    "\n",
    "range_n_clusters = []\n",
    "for i in range(2,201): # make a list from 1 till 31\n",
    "    range_n_clusters.append(i)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(V_penul) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(V_penul)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(V_penul, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(V_penul, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(V_penul[:, 0], V_penul[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = {} with Average Silhouette of: {}\".format(n_clusters,silhouette_avg)),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    pp.savefig(fig)\n",
    "\n",
    "plt.show()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T10:40:41.489903Z",
     "start_time": "2019-06-03T10:24:44.761460Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "#print(__doc__)\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "\n",
    "pp = PdfPages('TempMapping.pdf')\n",
    "\n",
    "\n",
    "range_n_clusters = []\n",
    "for i in range(2,101): # make a list from 1 till 31\n",
    "    range_n_clusters.append(i)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(a) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(V_penul)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(V_penul, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(V_penul, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(V_penul[:, 0], V_penul[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = {} with Average Silhouette of: {}\".format(n_clusters,silhouette_avg)),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    pp.savefig(fig)\n",
    "\n",
    "plt.show()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:32:54.340909Z",
     "start_time": "2019-05-30T12:20:02.172213Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "#print(__doc__)\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "\n",
    "pp = PdfPages('Test_V_standard.pdf')\n",
    "\n",
    "\n",
    "range_n_clusters = []\n",
    "for i in range(2,101): # make a list from 1 till 31\n",
    "    range_n_clusters.append(i)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(V_standard) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(V_standard)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(V_standard, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(V_standard, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(V_standard[:, 0], V_standard[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = {} with Average Silhouette of: {}\".format(n_clusters,silhouette_avg)),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    pp.savefig(fig)\n",
    "\n",
    "plt.show()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:00:06.168681Z",
     "start_time": "2019-06-07T11:00:06.161494Z"
    }
   },
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:00:07.451572Z",
     "start_time": "2019-06-07T11:00:07.447371Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences_full[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:00:08.070094Z",
     "start_time": "2019-06-07T11:00:08.063456Z"
    }
   },
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:00:15.711873Z",
     "start_time": "2019-06-07T11:00:15.705478Z"
    }
   },
   "outputs": [],
   "source": [
    "H_standard[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:00:21.029903Z",
     "start_time": "2019-06-07T11:00:21.023322Z"
    }
   },
   "outputs": [],
   "source": [
    "V_standard[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:01:44.126090Z",
     "start_time": "2019-06-07T11:01:44.122233Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_standard[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:01:45.105544Z",
     "start_time": "2019-06-07T11:01:45.099237Z"
    }
   },
   "outputs": [],
   "source": [
    "pred1_standard[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:18:56.004157Z",
     "start_time": "2019-06-07T11:18:55.865773Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = []\n",
    "for i in range(len(data)):\n",
    "    d.append({'Keywords': data[i], 'Sequence_Full': sequences_full[i], 'Prediction':pred_standard[i]})\n",
    "d = pd.DataFrame(d)\n",
    "#d.sort_values('Prediction')\n",
    "d.to_csv('CNN_Demo-HippoBased_Labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:02:33.415577Z",
     "start_time": "2019-06-07T11:02:33.164029Z"
    }
   },
   "outputs": [],
   "source": [
    "d.reset_index().plot.scatter(x='index', y='Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:02:34.489997Z",
     "start_time": "2019-06-07T11:02:34.347985Z"
    }
   },
   "outputs": [],
   "source": [
    "d[\"Prediction\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:02:35.184936Z",
     "start_time": "2019-06-07T11:02:35.156193Z"
    }
   },
   "outputs": [],
   "source": [
    "d.sort_values('Prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1069px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
